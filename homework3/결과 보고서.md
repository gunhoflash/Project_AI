# [2019-2 인공지능] 과제3 결과 보고서

2015920003 컴퓨터과학부 김건호



## 목차

1. 코드 설명
    1. 개요
    2. Neural Network
2. 실행 환경
3. 실행 결과
    1. 요약
    2. Error Graph
    3. Trained Network
4. 분석
    1. 학습 실패
    2. 효과적인 학습방법
       1. 비교
    3. 정규화



## 코드 설명

### 개요

- 4개의 Neural Network를 만들어 각각 AND-gate, OR-gate, XOR-gate, Donut의 동작을 하도록 학습시킨다.
- main함수에서는 학습에 사용될 데이터를 정의하고 Neural Network을 만들어 학습시킨다.
- 학습 결과는 외부 파일에 저장된다.

### Neural Network

- NeuralNetwork.h 헤더파일에서 Neural Network를 Class로 정의했으며 Calculate, Train 등 여러 메소드가 구현되어있다. Neural Network의 Weight는 RandomWeight 함수를 이용하여 Layer의 크기에 따라 랜덤한 실수(float)로 초기화된다.
- 학습 속도와 정확도를 개선하기 위해 Learning Rate Decay, Momentum 등을 적용했다.
- Neural Network는 Calculate로 계산한 결과를 softmax, one hot 등으로 정규화할 수 있는 옵션을 제공한다.
- 학습 과정에서 Network의 계산 오차는 Quadratic Loss(계산값과 기대값 차이의 제곱)으로 정의된다. 모든 학습 데이터셋에 대해 오차가 일정 수준 이하로 떨어질 때까지 학습을 반복하며, `1` epoch마다 최대 오차와 평균 오차를 외부 파일에 기록한다. 학습 횟수가 일정 수준을 넘으면 학습 실패로 간주하고 끝난다. 학습이 끝나면 Weight와 Threshold를 외부 파일에 기록한다.



## 실행 환경

| 구분          | 값                        |
| ------------- | ------------------------- |
| 운영체제      | Windows 10 1903           |
| 빌드 도구     | Visual Studio 2019 (v142) |
| C++ 언어 표준 | C++14                     |



## 실행 결과

### 요약

![[그림 1] Result: success](s.png)

[그림 1] Result: success



![[그림 2] Result: fail AND-gate](f_and.png)

[그림 2] Result: fail AND-gate



![[그림 3] Result: fail OR-gate](f_or.png)

[그림 3] Result: fail OR-gate



![[그림 4] Result: fail XOR-gate](f_xor.png)

[그림 4] Result: fail XOR-gate



![[그림 5] Result: fail Donut](f_dn.png)

[그림 5] Result: fail Donut



​	[그림 1], [그림 2], [그림 3], [그림 4], [그림 5]와 같이 AND-gate, OR-gate, XOR-gate, Donut의 학습은 성공하기도, 실패(학습 횟수가 `262144` epoch 이상)하기도 한다.

  각 Network는 높은 확률로 학습에 성공하는 경향을 보였다. 아주 가끔 학습 횟수가 `262144`번을 초과했는데, 특히 Donut에서 조금 더 빈번했다.



> #### 참고
>
> [그림 1], [그림 2], [그림 3], [그림 4], [그림 5]는 다음과 같은 기준을 두고 실행한 것이다.
>
> - momentum: `0.75`
> - loss tolerance: `0.02`
> - learning rate: `0.02` ~ `0.2`
> - output revision: `none`
> - layer size(Input Layer/Hidden Layers/Output Layer):
>   - AND-gate: `2/3/3/1`
>   - OR-gate: `2/3/1`
>   - XOR-gate: `2/4/1`
>   - Donut: `2/3/3/3/1`

> #### 참고
>
>   [그림 1]에서 각 Neural Network의 학습 후 최종 Weight는 파일로 별도 첨부했다. 각 Weight는 `[i][j][k] #`형식으로 기록되어있는데, 이는 `i`번째 Layer의 `k`번째 Perceptron과 `i+1`번째 Layer의 `j`번째 Perceptron을 잇는 Edge의 Weight 값을 의미한다.



### Error Graph

[그림 1]의 경우에서 각 epoch 당 Quadratic Loss는 다음과 같다.



![[그림 6] Error: success AND-gate](error_s_and.png)

[그림 6] Error: success AND-gate



![[그림 7] Error: success OR-gate](error_s_or.png)

[그림 7] Error: success OR-gate



![[그림 8] Error: success XOR-gate](error_s_xor.png)

[그림 8] Error: success XOR-gate



![[그림 9] Error: success Donut](error_s_dn.png)

[그림 9] Error: success Donut



[그림 2], [그림 3], [그림 4], [그림 5]의 학습 실패(학습 횟수가 `262144` epoch 이상) 경우에서 각 epoch 당 Quadratic Loss는 다음과 같다.



![[그림 10] Error: fail AND-gate](error_and.png)

[그림 10] Error: fail AND-gate



![[그림 11] Error: fail OR-gate](error_or.png)

[그림 11] Error: fail OR-gate



![[그림 12] Error: fail XOR-gate](error_xor.png)

[그림 12] Error: fail XOR-gate



![[그림 13] Error: fail Donut](error_dn.png)

[그림 13] Error: fail Donut



> #### 참고
>
> 활성화함수로 ELU를 선택했고 softmax나 one hot 등으로 정규화하지 않았기 때문에 Loss 값이 `1`보다 클 수 있다.



### Trained Network

각 Network에서 입력 $<x, y> (x, y \in \{0, 0.1, 0.2, 0.3, ..., 1\})$에 대한 연산 결과는 다음과 같다.



![[그림 14] Trained Network: AND-gate](AND1.png)

[그림 14] Trained Network: AND-gate



![[그림 15] Trained Network: AND-gate](AND2.png)

[그림 15] Trained Network: AND-gate



![[그림 16] Trained Network: OR-gate](OR1.png)

[그림 16] Trained Network: OR-gate



![[그림 17] Trained Network: OR-gate](OR2.png)

[그림 17] Trained Network: OR-gate



![[그림 18] Trained Network: XOR-gate](XOR1.png)

[그림 18] Trained Network: XOR-gate



![[그림 19] Trained Network: XOR-gate](XOR2.png)

[그림 19] Trained Network: XOR-gate



![[그림 20] Trained Network: Donut](DONUT1.png)

[그림 20] Trained Network: Donut



![[그림 21] Trained Network: Donut](DONUT2.png)

[그림 21] Trained Network: Donut



## 분석

### 학습 실패

  학습 횟수가 `262144`번 이상이 되는 경우 학습을 중단하고 실패로 간주한다. 학습 실패가 발생하는 경우는 드물었고, loss의 값이 **매우 천천히 감소한다**는 공통점이 있었다. 이것이 학습을 뒤늦게 성공하는 것인지, 아니면 결국 목표에 도달하지 못하는 것인지는 학습 횟수 제한을 없앰으로써 확인해볼 수 있지만, 이 경우 감당하기 힘들 정도로 오랜 시간이 걸릴 수 있다. 단, 횟수 제한을 `1048576`처럼 조금 더 키웠을 때, 학습에 성공하는 사례가 간혹 나오긴 한 것으로 보아, 위 학습 실패 사례는 모두 뒤늦게라도 학습에 성공할 것으로 기대된다.

  learning rate, momentum, threshold를 변경해도 해당 문제는 발생한다. activation function을 sigmoid, tanh, ReLU, Leaky ReLU, ELU 등으로 적용했을 때 ELU가 제일 안정적이고 좋은 성능을 보였다. 결국, **activation function과 초기 weight의 값에 따라 학습시간이 크게 달라질 수 있다**는 결론을 내렸다.



### 효과적인 학습방법

  학습 횟수를 줄이거나 시간을 단축하기 위해 내 코드에 적용한 방법은 다음과 같다.

- Learnng Rate Decay
  
    - Learnng Rate의 초기값을 크게 하고 Epoch의 증가에 따라 이를 감소시킴으로써 학습 속도와 정확도를 높였다.
- Momentum
  
    - Weight 값들이 경사하강법에 의해 변화할 때, Momentum을 줌으로써 학습 속도를 높였다. 일시적으로 Loss가 상승할 수 있지만, Weight 값이 목표를 향해 더욱 빠르게 변할 수 있고 Loss가 극소인 지점에 쉽게 머무르지 않는다.
- ELU
  
    - Activation Function을 ELU로 선택했다. 이를 통해 sigmoid 사용 시 나타나는 Gradient Vanishing 문제를 해결할 수 있다. 하지만 Gradient Exploding 문제가 생길 수 있고, 내 코드에서 이를 예방하지는 않았다. 다만, 각 Weight가 `0`에 가까울 수록 Gradient Exploding가 발생하지 않는다.
- Small Random Weight

    - 초기 Weight 값은 학습 횟수와 방향에 큰 영향을 끼친다. 특히 Weight의 절댓값이 너무 크면 Back Propagation 단계에서 계산되는 Delta의 절댓값 또한 커지게 되어 Gradient Exploding로 이어질 수 있다. 따라서 나는 초기 Weight를 `0`에 매우 가까운 수로 설정했다.
- No Bias
    - 내 코드에는 Bias(Threshold)가 구현되어 있지만 이를 Weight처럼 학습시키는 것까지는 구현하지 않았다. 사실 Bias를 사용하지 않을(`0`으로 고정할) 계획이었기 때문이다. 원래는 Bias를 사용해야 하지만, 위 4가지의 Neural Network들은 우연히도 모두 입력이 `0,0`일때 원하는 출력이 `0`이기 때문에 Bias를 `0`으로 고정해도 문제가 발생하지 않는다. 하지만 입력이 `0`으로만 구성될 때 출력이 `0`이 아닌 값이 되길 원하는 상황에서는 반드시 Bias를 사용해야 할 것이다.



  위의 학습 과정에서 AND-gate, OR-gate, XOR-gate는 보통 수십-수백 epoch만에 학습을 완료했고, Donut은 보통 수백-수천 epoch만에 학습을 완료했다.



#### 비교

  [그림 22], [그림 23], [그림 24], [그림 25], [그림 26]은 기존 코드에서 Learning Rate를 `0.05`로 고정하고 Momentum을 `0`으로 설정했을 때의 학습 결과이고,  [그림 27], [그림 28], [그림 29], [그림 30], [그림 31]은 기존 코드에서 Activation Function을 sigmoid로 변경했을 때의 학습 결과다. 두 결과 모두 기존 코드보다 학습 횟수가 훨씬 컸고, 특히 Donut의 학습이 매우 힘들었다.

![[그림 22] Result: success: static leraning rate, no momentum](s_m0_l005.png)

[그림 22] Result: success: static leraning rate, no momentum



![[그림 23] Error: success AND-gate: static leraning rate, no momentum](error_s_m0_l005_and.png)

[그림 23] Error: success AND-gate: static leraning rate, no momentum



![[그림 24] Error: success OR-gate: static leraning rate, no momentum](error_s_m0_l005_or.png)

[그림 24] Error: success OR-gate: static leraning rate, no momentum



![[그림 25] Error: success XOR-gate: static leraning rate, no momentum](error_s_m0_l005_xor.png)

[그림 25] Error: success XOR-gate: static leraning rate, no momentum



![[그림 26] Error: success Donut: static leraning rate, no momentum](error_s_m0_l005_dn.png)

[그림 26] Error: success Donut: static leraning rate, no momentum



![[그림 27] Result: sigmoid](sigmoid.png)

[그림 27] Result: sigmoid



![[그림 28] Error: AND-gate: sigmoid](error_sigmoid_and.png)

[그림 28] Error: AND-gate: sigmoid



![[그림 29] Error: OR-gate: sigmoid](error_sigmoid_or.png)

[그림 29] Error: OR-gate: sigmoid



![[그림 30] Error: XOR-gate: sigmoid](error_sigmoid_xor.png)

[그림 30] Error: XOR-gate: sigmoid



![[그림 31] Error: Donut: sigmoid](error_sigmoid_dn.png)

[그림 31] Error: Donut: sigmoid



### 정규화

  내 코드는 network의 계산 결과를 정규화할 수 있는 옵션(softmax와 one hot)을 제공한다. 만약 가능한 모든 입력이 학습에 쓰이고, network 사용자가 정규화된 계산 결과만 이용한다면, network의 학습 횟수를 크게 단축시킬 수 있다.

  현재 내 코드는 모든 학습데이터에 대해 계산 결과의 Loss가 일정 수준 이하일 때 학습 성공으로 간주한다. 이때 계산 결과는 정규화되지 않은 것으로, activation function에 따라 그 값의 범위가 달라진다. 하지만 정규화된 결과로 Loss를 비교한다면, 이는 학습 성공의 기준을 크게 완화하는 것이다. (물론 Back Propagation을 진행할 때는 정규화되기 전의 결과를 이용한다.) 이 방법을 적용하면 [그림 10], [그림 11]과 같은 상황이 발생하지 않는다.

  단, 이 방법은 오로지 AND-gate, OR-gate, XOR-gate, Donut처럼 가능한 모든 입력값이 학습과정에 쓰이고, 정규화된 결과(이 경우 0 또는 1)만 이용할 때 의미가 있을 것이다.
